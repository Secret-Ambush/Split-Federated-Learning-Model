# -*- coding: utf-8 -*-
"""gpu_efficientNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zV9skkxIF53HYZdGC96cMO0GFrr_WpcO

# Optimized EfficientNet Split Federated Learning with GPU
## Enhanced with GPU optimizations and robust checkpoint/resume functionality
"""

# from google.colab import drive
# drive.mount('/content/drive')

# GPU Setup and Optimizations
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision.models as models
import torchvision.transforms as transforms
import numpy as np
import random
import pandas as pd
import os
import json
import pickle
from datetime import datetime
from torch.utils.data import random_split, DataLoader, Subset
from PIL import Image
import matplotlib.pyplot as plt
from torchvision.utils import make_grid
from torchvision.transforms import ToPILImage
import csv
from collections import Counter
from torch.utils.data import WeightedRandomSampler

# GPU optimization settings
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For better error reporting
torch.backends.cudnn.benchmark = True  # Optimize for fixed input sizes
torch.backends.cudnn.deterministic = False  # Allow non-deterministic algorithms for speed

# Check for GPU availability
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")

class SplitEfficientNetB0(nn.Module):
    def __init__(self, num_classes):
        nn.Module.__init__(self)  # Direct call to parent constructor
        base_model = models.efficientnet_b0(weights=None)
        # Access the in_features from the Linear layer within the Sequential classifier
        # EfficientNetB0 classifier is Sequential(Dropout, Linear)
        self.features = base_model.features
        self.avgpool = base_model.avgpool
        self.classifier_linear = base_model.classifier[1] # Access the Linear layer

        # Split EfficientNet into 3 blocks (indices are approximate and depend on the specific model architecture)
        # Refer to torchvision source code for exact layer indices if needed
        self.block1 = self.features[:3]  # Initial conv + early blocks
        self.block2 = self.features[3:5] # Middle blocks
        self.block3 = self.features[5:]  # Later blocks + final conv

    def forward_until(self, x, cut_layer):
        if cut_layer == 0:
            return x
        elif cut_layer == 1:
            return self.block1(x)
        elif cut_layer == 2:
            x = self.block1(x)
            return self.block2(x)
        elif cut_layer == 3:
            x = self.block1(x)
            x = self.block2(x)
            return self.block3(x)
        elif cut_layer == -1:
            return x # This case should not be reached with current cut_layers, but kept for consistency
        else:
            raise ValueError("Invalid cut layer")

    def forward_from(self, x, cut_layer):
        if cut_layer == 0:
            x = self.block1(x)
            x = self.block2(x)
            x = self.block3(x)
        elif cut_layer == 1:
            x = self.block2(x)
            x = self.block3(x)
        elif cut_layer == 2:
            x = self.block3(x)
        elif cut_layer == 3:
            pass # Input is already the output of block3
        elif cut_layer == -1:
            # This case handles the transition from features to classifier
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            return self.classifier_linear(x)
        else:
             raise ValueError("Invalid cut layer")

        # If cut_layer is 0, 1, or 2, the forward_until will handle the blocks
        # Then we apply the remaining blocks and the classifier
        x = self.avgpool(x)
        x = torch.flatten(x, 1)
        return self.classifier_linear(x)

# Configuration class (simplified)
class ExperimentConfig:
    def __init__(self):
        # GPU Optimized Parameters
        self.NUM_CLIENTS = 10
        self.ALPHA = 0.5
        self.NUM_ROUNDS = 30
        self.LOCAL_EPOCHS = 5
        self.BATCH_SIZE = 64  # Optimized for GPU memory
        self.LEARNING_RATE = 0.01

        # Model Configuration
        self.MODEL_LIST = [("EfficientNetB0", SplitEfficientNetB0)]

        # File Paths
        self.LOG_PATH = "~/Desktop/Riddhi/efficientNet_accuracy_log.txt"
        self.CSV_PATH = "~/Desktop/drive/MyDrive/Riddhi/final_dataset.csv"
        self.FINAL_OUTPUT_PATH = "~/Desktop/Riddhi/efficientNet_finalResults.csv"
        self.CHECKPOINT_DIR = "~/Desktop/Riddhi/efficientnet_checkpoints"
        self.PROGRESS_FILE = "~/Desktop/Riddhi/efficientnet_progress.json"

        # Attack Parameters
        self.INJECTION_RATE = 0.5
        self.PATTERN_SIZE = 0.1
        self.TARGET_LABEL = 1
        self.ATTACKER_PERCENTAGES = [20, 50]
        self.CUT_LAYERS = [0, 1, 2, 3, -1]

        # GPU Optimizations
        self.USE_MIXED_PRECISION = True
        self.NUM_WORKERS = 4  # Optimized for GPU
        self.PREFETCH_FACTOR = 2

        # Configurations
        self.configurations = [
            {"label": "Static case", "location": "fixed", "pattern_type": "plus"},
            {"label": "Size Invariant", "location": "fixed", "pattern_type": "plus", "pattern_size": "random"},
            {"label": "Pattern Invariant", "location": "fixed", "pattern_type": "random"},
            {"label": "Random accross all", "location": "fixed", "pattern_type": "random", "pattern_size": "random"}
        ]

# Initialize configuration
config = ExperimentConfig()

# Enhanced Progress Tracking
class ProgressTracker:
    def __init__(self, progress_file):
        self.progress_file = progress_file
        self.completed_configs = set()
        self.current_config = None
        self.load_progress()

    def load_progress(self):
        """Load progress from file"""
        if os.path.exists(self.progress_file):
            try:
                with open(self.progress_file, 'r') as f:
                    data = json.load(f)
                    self.completed_configs = set(tuple(config) for config in data.get('completed_configs', []))
                    self.current_config = data.get('current_config')
                print(f"Loaded progress: {len(self.completed_configs)} completed configurations")
            except Exception as e:
                print(f"Error loading progress: {e}")
                self.completed_configs = set()
        else:
            print("No progress file found. Starting fresh.")

    def save_progress(self):
        """Save progress to file"""
        try:
            data = {
                'completed_configs': [list(config) for config in self.completed_configs],
                'current_config': self.current_config,
                'last_updated': datetime.now().isoformat()
            }
            with open(self.progress_file, 'w') as f:
                json.dump(data, f, indent=2)
        except Exception as e:
            print(f"Error saving progress: {e}")

    def is_completed(self, model_name, cut_layer, config_label, attacker_percentage):
        """Check if configuration is completed"""
        config_key = (model_name, cut_layer, config_label, attacker_percentage)
        return config_key in self.completed_configs

    def mark_completed(self, model_name, cut_layer, config_label, attacker_percentage):
        """Mark configuration as completed"""
        config_key = (model_name, cut_layer, config_label, attacker_percentage)
        self.completed_configs.add(config_key)
        self.save_progress()

    def set_current_config(self, model_name, cut_layer, config_label, attacker_percentage):
        """Set current configuration being processed"""
        self.current_config = (model_name, cut_layer, config_label, attacker_percentage)
        self.save_progress()

    def get_next_config(self, model_name, ModelClass, config):
        """Get next configuration to process"""
        cut_layers = get_cut_layers_for_model(ModelClass(num_classes=13))  # Assuming 13 classes

        for cut_layer in cut_layers:
            for config_dict in config.configurations:
                for perc in config.ATTACKER_PERCENTAGES:
                    if not self.is_completed(model_name, cut_layer, config_dict["label"], perc):
                        return model_name, cut_layer, config_dict, perc
        return None

    def get_starting_point(self, model_name, ModelClass, config):
        """Get the starting point for processing - either from beginning or after last completed config"""
        cut_layers = get_cut_layers_for_model(ModelClass(num_classes=13))

        # Find the last completed configuration
        last_completed = None

        for cut_layer in cut_layers:
            for config_dict in config.configurations:
                for perc in config.ATTACKER_PERCENTAGES:
                    if self.is_completed(model_name, cut_layer, config_dict["label"], perc):
                        last_completed = (cut_layer, config_dict, perc)
                    else:
                        # Found first incomplete config - this is our starting point
                        if last_completed is None:
                            # No previous completed configs, start from beginning
                            return cut_layer, config_dict, perc, True
                        else:
                            # Start from the next config after last completed
                            return cut_layer, config_dict, perc, False

        # All configs completed
        return None, None, None, False

    def get_configuration_order(self, model_name, ModelClass, config):
        """Get the complete order of configurations to process"""
        cut_layers = get_cut_layers_for_model(ModelClass(num_classes=13))
        config_order = []

        for cut_layer in cut_layers:
            for config_dict in config.configurations:
                for perc in config.ATTACKER_PERCENTAGES:
                    config_order.append((cut_layer, config_dict, perc))

        return config_order

    def find_next_incomplete_config(self, model_name, ModelClass, config):
        """Find the next incomplete configuration in the sequence"""
        config_order = self.get_configuration_order(model_name, ModelClass, config)

        for cut_layer, config_dict, perc in config_order:
            if not self.is_completed(model_name, cut_layer, config_dict["label"], perc):
                return cut_layer, config_dict, perc

        return None, None, None

# Initialize progress tracker
progress_tracker = ProgressTracker(config.PROGRESS_FILE)

def get_cut_layers_for_model(model):
    """
    Dynamically determines cut layers based on model definition.
    Also includes two extremes:
    - 0  = full model on client
    - -1 = client does minimal work, server does most
    """
    cut_layers = []

    if hasattr(model, 'forward_until'):
        import inspect
        try:
            src = inspect.getsource(model.forward_until)
            # Look for patterns like 'cut_layer == X' where X is a number
            import re
            matches = re.findall(r"cut_layer == (\d+)", src)
            cut_layers = sorted(list(set(int(m) for m in matches)))
        except Exception as e:
            print(f"Warning: Could not dynamically determine cut layers. Using default [1, 2, 3]. Error: {e}")
            # Fallback to a default list if inspection fails
            cut_layers = [1, 2, 3]


    # Ensure 0 and -1 are included
    if 0 not in cut_layers:
        cut_layers = [0] + cut_layers
    if -1 not in cut_layers:
        cut_layers = cut_layers + [-1]

    return sorted(list(set(cut_layers))) # Return sorted unique layers

def inject_backdoor_dynamic(data, targets, injection_rate=0.5, pattern_type="plus",
                            pattern_size=0.1, location="fixed", target_label=1,
                            color=(0.5, 0.0, 0.5)):  # Default: purple
    """
    Injects a dynamic backdoor trigger into a fraction of images in the batch.

    Parameters:
      data (torch.Tensor): Batch of images, shape (B, C, H, W).
      targets (torch.Tensor): Corresponding labels.
      injection_rate (float): Fraction of images to modify.
      pattern_type (str): 'plus', 'minus', 'block', or 'random'.
      pattern_size (float): Fraction of image dimension to determine patch size.
      location (str): 'fixed' or 'random' placement.
      target_label (int): The label to assign to backdoor images.
      color (tuple): RGB tuple with values between 0-1 for patch colour (e.g. purple = (0.5, 0, 0.5)).

    Returns:
      (data, targets): Modified tensors.
    """
    B, C, H, W = data.shape
    num_to_inject = int(B * injection_rate)
    if num_to_inject == 0:
        return data, targets

    indices = torch.randperm(B)[:num_to_inject]

    for i in indices:
        ps = random.choice([0.1, 0.2, 0.3, 0.4]) if pattern_size == -1 else pattern_size
        s = max(int(H * ps), 1)

        if location == "random":
            top = torch.randint(0, H - s + 1, (1,)).item()
            left = torch.randint(0, W - s + 1, (1,)).item()
        else:  # fixed
            top = H - s
            left = W - s

        actual_pattern = pattern_type
        if pattern_type == "random":
            actual_pattern = random.choice(["plus", "minus", "block"])

        r, g, b = color

        if actual_pattern == "plus":
            center_row = top + s // 2
            center_col = left + s // 2
            data[i, 0, center_row, left:left + s] = r  # Red channel horizontal
            data[i, 1, center_row, left:left + s] = g
            data[i, 2, center_row, left:left + s] = b
            data[i, 0, top:top + s, center_col] = r  # Red channel vertical
            data[i, 1, top:top + s, center_col] = g
            data[i, 2, top:top + s, center_col] = b

        elif actual_pattern == "minus":
            center_row = top + s // 2
            data[i, 0, center_row, left:left + s] = r
            data[i, 1, center_row, left:left + s] = g
            data[i, 2, center_row, left:left + s] = b

        elif actual_pattern == "block":
            data[i, 0, top:top + s, left:left + s] = r
            data[i, 1, top:top + s, left:left + s] = g
            data[i, 2, top:top + s, left:left + s] = b

        else:  # default to plus
            center_row = top + s // 2
            center_col = left + s // 2
            data[i, 0, center_row, left:left + s] = r
            data[i, 1, center_row, left:left + s] = g
            data[i, 2, center_row, left:left + s] = b
            data[i, 0, top:top + s, center_col] = r
            data[i, 1, top:top + s, center_col] = g
            data[i, 2, top:top + s, center_col] = b

        targets[i] = target_label

    return data, targets

class TrafficSignDataset(torch.utils.data.Dataset):
    def __init__(self, df, transform=None, label_map=None):
        self.df = df
        self.transform = transform or transforms.ToTensor()

        # Ensure consistent label encoding (string to int)
        if label_map is None:
            self.label_map = {label: idx for idx, label in enumerate(sorted(df['class'].unique()))}
        else:
            self.label_map = label_map

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        image = Image.open(row['image_path']).convert('RGB')
        label = self.label_map[row['class']]
        if self.transform:
            image = self.transform(image)
        return image, label

def get_custom_dataset(csv_path, resize_shape=(32, 32), split_ratio=0.8):
    df = pd.read_csv(csv_path)
    df = df[~df['image_path'].str.endswith('.DS_Store')]

    transform = transforms.Compose([
        transforms.Resize(resize_shape),
        transforms.ToTensor()
    ])

    label_map = {label: idx for idx, label in enumerate(sorted(df['class'].unique()))}
    dataset = TrafficSignDataset(df, transform=transform, label_map=label_map)

    train_size = int(len(dataset) * split_ratio)
    test_size = len(dataset) - train_size
    trainset, testset = random_split(dataset, [train_size, test_size])

    num_classes = len(label_map)
    return trainset, testset, num_classes

def split_dataset_dirichlet(dataset, num_clients, alpha):
    # For custom datasets with label mapping
    if hasattr(dataset.dataset, 'df'):
        labels = [dataset.dataset.label_map[row['class']] for i in dataset.indices for _, row in dataset.dataset.df.iloc[[i]].iterrows()]
    elif hasattr(dataset, 'targets'):
        labels = dataset.targets
    elif hasattr(dataset, 'labels'):
        labels = dataset.labels
    else:
        raise ValueError("Cannot extract labels from dataset.")

    labels = np.array(labels)
    num_classes = np.unique(labels).size
    idx_by_class = {k: np.where(labels == k)[0] for k in range(num_classes)}

    client_indices = {i: [] for i in range(num_clients)}
    for c in range(num_classes):
        idx_c = idx_by_class[c]
        np.random.shuffle(idx_c)
        proportions = np.random.dirichlet(alpha * np.ones(num_clients))
        proportions = (np.cumsum(proportions) * len(idx_c)).astype(int)
        proportions = np.concatenate(([0], proportions))
        for i in range(num_clients):
            client_indices[i].extend(idx_c[proportions[i]:proportions[i + 1]])

    return client_indices

def server_aggregate_split(global_model, client_state_dicts):
    """
    Aggregates the parameters of the client-side model across all clients
    using simple averaging.
    """
    global_dict = global_model.state_dict()

    for key in global_dict.keys():
        stacked = torch.stack(
            [sd[key].detach().float() for sd in client_state_dicts],
            dim=0
        ).mean(0)
        global_dict[key] = stacked

    global_model.load_state_dict(global_dict)
    return global_model

LOG_PATH = "/content/drive/MyDrive/Riddhi/efficientnet_accuracy_log.txt"

def log_accuracy_to_file(model_name, cut_layer, accuracy):
    with open(LOG_PATH, "a") as f:
        f.write(f"{model_name}, Cut Layer {cut_layer}, Accuracy: {accuracy:.2f}%\n")

def log_metrics_to_file(model_name, cut_layer, clean_accuracy, backdoor_accuracy, attack_success_rate, config, num_attackers):
    with open(LOG_PATH, "a") as f:
        f.write(
            f"{model_name}, Config: {config}, Cut Layer {cut_layer}, Attackers: {num_attackers}, "
            f"Clean Acc: {clean_accuracy:.2f}%, Backdoor Acc: {backdoor_accuracy:.2f}%, ASR: {attack_success_rate:.2f}%\n"
        )

# GPU-optimized training function
def splitfl_train_epoch_optimized(client_model, server_model, dataloader, cut_layer, lr, device,
                                 malicious=False, injection_rate=0.5, pattern_size=0.1,
                                 location="fixed", pattern_type="plus", target_label=1):
    """GPU-optimized one epoch of SplitFed training."""
    criterion = nn.CrossEntropyLoss()
    client_model.train()
    server_model.train()

    optimizer = optim.SGD(list(client_model.parameters()) + list(server_model.parameters()),
                         lr=lr, momentum=0.9, weight_decay=1e-4)

    total_loss = 0
    num_batches = 0

    for batch_idx, (data, target) in enumerate(dataloader):
        data = data.to(device, non_blocking=True)
        target = target.to(device, non_blocking=True)

        if malicious:
            data, target = inject_backdoor_dynamic(
                data, target, injection_rate=injection_rate,
                pattern_type=pattern_type, pattern_size=pattern_size,
                location=location, target_label=target_label
            )

        optimizer.zero_grad()

        activation = client_model.forward_until(data, cut_layer)
        output = server_model.forward_from(activation, cut_layer)
        loss = criterion(output, target)

        loss.backward()
        optimizer.step()

        total_loss += loss.detach()
        num_batches += 1

    return (total_loss / num_batches).cpu().item()

# Enhanced checkpoint management
class CheckpointManager:
    def __init__(self, checkpoint_dir):
        self.checkpoint_dir = checkpoint_dir
        os.makedirs(checkpoint_dir, exist_ok=True)

    def clear_all_checkpoints(self):
        """Delete all previous checkpoints to prevent multiple checkpoints"""
        if os.path.exists(self.checkpoint_dir):
            for filename in os.listdir(self.checkpoint_dir):
                if filename.endswith('.pth'):
                    file_path = os.path.join(self.checkpoint_dir, filename)
                    try:
                        os.remove(file_path)
                        print(f"Deleted checkpoint: {filename}")
                    except Exception as e:
                        print(f"Error deleting {filename}: {e}")

    def get_checkpoint_path(self, model_name, cut_layer, num_attackers, config_label):
        """Generate checkpoint file path"""
        config_label_cleaned = config_label.replace(' ', '_').replace(':', '')
        filename = f"{model_name}_cutlayer{cut_layer}_attackers{num_attackers}_{config_label_cleaned}.pth"
        return os.path.join(self.checkpoint_dir, filename)

    def save_checkpoint(self, model_name, cut_layer, num_attackers, config_label,
                       client_model, server_model, round_num, metrics=None):
        """Save checkpoint with enhanced metadata"""
        checkpoint_path = self.get_checkpoint_path(model_name, cut_layer, num_attackers, config_label)

        checkpoint_data = {
            'round': round_num,
            'client_model_state_dict': client_model.state_dict(),
            'server_model_state_dict': server_model.state_dict(),
            'config': config_label,
            'num_attackers': num_attackers,
            'cut_layer': cut_layer,
            'model_name': model_name,
            'timestamp': datetime.now().isoformat(),
            'metrics': metrics or {}
        }

        torch.save(checkpoint_data, checkpoint_path)
        print(f"Checkpoint saved: {checkpoint_path}")

    def load_checkpoint(self, model_name, cut_layer, num_attackers, config_label, device):
        """Load checkpoint with error handling"""
        checkpoint_path = self.get_checkpoint_path(model_name, cut_layer, num_attackers, config_label)

        if os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location=device)
                print(f"Loaded checkpoint from round {checkpoint['round']}")
                return checkpoint
            except Exception as e:
                print(f"Error loading checkpoint: {e}")
                return None
        return None

    def get_latest_checkpoint_info(self, model_name, cut_layer, num_attackers, config_label):
        """Get information about the latest checkpoint"""
        checkpoint_path = self.get_checkpoint_path(model_name, cut_layer, num_attackers, config_label)

        if os.path.exists(checkpoint_path):
            try:
                checkpoint = torch.load(checkpoint_path, map_location='cpu')
                return {
                    'round': checkpoint['round'],
                    'timestamp': checkpoint.get('timestamp', 'Unknown'),
                    'metrics': checkpoint.get('metrics', {})
                }
            except:
                return None
        return None

# Enhanced CSV Result Manager
class CSVResultManager:
    def __init__(self, csv_path):
        self.csv_path = csv_path
        self.results = []
        self.load_existing_results()

    def load_existing_results(self):
        """Load existing results from CSV file if it exists"""
        if os.path.exists(self.csv_path):
            try:
                existing_df = pd.read_csv(self.csv_path)
                self.results = existing_df.to_dict('records')
                print(f"Loaded {len(self.results)} existing results from {self.csv_path}")
            except Exception as e:
                print(f"Error loading existing CSV: {e}")
                self.results = []
        else:
            print(f"No existing CSV found. Creating new file: {self.csv_path}")

    def add_result(self, result_dict):
        """Add a new result to the list"""
        self.results.append(result_dict)

    def save_results(self):
        """Save all results to CSV file"""
        if self.results:
            df = pd.DataFrame(self.results)
            df.to_csv(self.csv_path, index=False)
            print(f"Results saved to {self.csv_path} ({len(self.results)} total results)")

    def append_result(self, result_dict):
        """Add result and immediately append to CSV file"""
        self.add_result(result_dict)
        df = pd.DataFrame([result_dict])
        df.to_csv(self.csv_path, mode='a', header=not os.path.exists(self.csv_path), index=False)
        print(f"Result appended to {self.csv_path}")

# Initialize managers
checkpoint_manager = CheckpointManager(config.CHECKPOINT_DIR)
csv_manager = CSVResultManager(config.FINAL_OUTPUT_PATH)

def initialize_models_for_gpu(ModelClass, num_classes, device):
    """Initialize models for GPU"""
    client_model = ModelClass(num_classes=num_classes)
    server_model = ModelClass(num_classes=num_classes)

    # Move to device
    client_model = client_model.to(device)
    server_model = server_model.to(device)

    return client_model, server_model

# Enhanced backdoor experiment with GPU optimizations
def run_backdoor_experiment_optimized(ModelClass, num_attackers, config_dict, cut_layer, model_name, device):
    print(f"\n--- Running GPU-Optimized Backdoor Experiment: {model_name} | Cut Layer: {cut_layer} ---")
    print(f"Attack Config: {config_dict} | Num Attackers: {num_attackers}")

    # Initialize models
    client_model, server_model = initialize_models_for_gpu(ModelClass, 13, device)

    # Load checkpoint if exists
    checkpoint = checkpoint_manager.load_checkpoint(
        model_name, cut_layer, num_attackers, config_dict['label'], device
    )

    start_round = 0
    if checkpoint:
        client_model.load_state_dict(checkpoint['client_model_state_dict'])
        server_model.load_state_dict(checkpoint['server_model_state_dict'])
        start_round = checkpoint['round'] + 1
        print(f"Resuming from round {start_round}")
    else:
        print(f"Starting new training for {config_dict['label']}")

    # Extract attack parameters
    pattern_size = -1 if config_dict.get("pattern_size") == "random" else config_dict.get("pattern_size", config.PATTERN_SIZE)
    location = config_dict["location"]
    pattern_type = config_dict["pattern_type"]

    # Prepare data loaders with GPU optimizations
    trainset, testset, num_classes = get_custom_dataset(config.CSV_PATH, (224, 224))
    client_indices = split_dataset_dirichlet(trainset, config.NUM_CLIENTS, config.ALPHA)

    all_labels = [trainset.dataset.label_map[row["class"]] for _, row in trainset.dataset.df.iterrows()]
    class_counts = Counter(all_labels)

    # Training loop with enhanced GPU optimizations
    for rnd in range(start_round, config.NUM_ROUNDS):
        print(f"\n[Round {rnd + 1}/{config.NUM_ROUNDS}] Training clients...")
        client_state_dicts = []
        malicious_clients = random.sample(range(config.NUM_CLIENTS), num_attackers)

        for client_id in range(config.NUM_CLIENTS):
            print(f"  â†’ Client {client_id} [{'Malicious' if client_id in malicious_clients else 'Benign'}]")

            indices = client_indices[client_id]
            client_data = Subset(trainset, indices)

            client_labels = [trainset.dataset.label_map[trainset.dataset.df.iloc[i]["class"]] for i in indices]
            sample_weights = [1.0 / class_counts[label] for label in client_labels]
            sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)

            # GPU-optimized data loader
            loader = DataLoader(
                client_data,
                batch_size=config.BATCH_SIZE,
                sampler=sampler,
                drop_last=True,
                num_workers=config.NUM_WORKERS,
                pin_memory=True
            )

            # Local models
            local_client = ModelClass(num_classes=num_classes).to(device)
            local_server = ModelClass(num_classes=num_classes).to(device)
            local_client.load_state_dict(client_model.state_dict())
            local_server.load_state_dict(server_model.state_dict())

            # GPU-optimized training
            loss = splitfl_train_epoch_optimized(
                local_client, local_server, loader, cut_layer,
                lr=config.LEARNING_RATE, device=device,
                malicious=(client_id in malicious_clients),
                injection_rate=config.INJECTION_RATE,
                pattern_type=pattern_type, pattern_size=pattern_size,
                location=location, target_label=config.TARGET_LABEL
            )

            client_state_dicts.append(local_client.state_dict())
            print(f"    Loss: {loss:.4f}")

        # Aggregation
        client_model = server_aggregate_split(client_model, client_state_dicts)
        server_model.load_state_dict(client_model.state_dict())
        print("  â†ª Aggregation complete.")

        # Save checkpoint with metrics (only if not the final round)
        if rnd < config.NUM_ROUNDS - 1:
            metrics = {
                'round': rnd,
                'loss': loss,
                'num_attackers': num_attackers,
                'config': config_dict
            }
            checkpoint_manager.save_checkpoint(
                model_name, cut_layer, num_attackers, config_dict['label'],
                client_model, server_model, rnd, metrics
            )

    # Enhanced evaluation with GPU optimizations
    print("\n--- Evaluating model on clean and backdoored test data ---")
    testloader = DataLoader(
        testset,
        batch_size=config.BATCH_SIZE,
        shuffle=False,
        drop_last=True,
        num_workers=config.NUM_WORKERS,
        pin_memory=True
    )

    client_model.eval()
    server_model.eval()

    with torch.no_grad():
        total = 0
        correct_clean = 0
        correct_bd = 0
        total_non_target = 0
        successful_attacks = 0

        for data, target in testloader:
            data = data.to(device, non_blocking=True)
            target = target.to(device, non_blocking=True)
            original_target = target.clone()

            bd_data, _ = inject_backdoor_dynamic(
                data.clone(), target.clone(),
                injection_rate=1.0, pattern_type=pattern_type,
                pattern_size=pattern_size, location=location,
                target_label=config.TARGET_LABEL
            )

            # Standard evaluation
            out_bd = server_model.forward_from(
                client_model.forward_until(bd_data, cut_layer), cut_layer
            )
            _, pred_bd = torch.max(out_bd, 1)

            out_clean = server_model.forward_from(
                client_model.forward_until(data, cut_layer), cut_layer
            )
            _, pred_clean = torch.max(out_clean, 1)

            correct_clean += (pred_clean == original_target).sum().item()
            correct_bd += (pred_bd == original_target).sum().item()

            mask = original_target != config.TARGET_LABEL
            total_non_target += mask.sum().item()
            successful_attacks += ((pred_bd == config.TARGET_LABEL) & mask).sum().item()

            total += data.size(0)

    asr = 100 * successful_attacks / max(total_non_target, 1)
    cleanAcc = 100 * correct_clean / total
    backAcc = 100 * correct_bd / total

    print("\n--- Final Metrics ---")
    print(f"Clean Accuracy: {cleanAcc:.2f}%")
    print(f"Backdoor Accuracy: {backAcc:.2f}%")
    print(f"Attack Success Rate (ASR): {asr:.2f}%")

    # Log metrics
    log_metrics_to_file(model_name, cut_layer, cleanAcc, backAcc, asr, config_dict, num_attackers)

    return asr, backAcc, cleanAcc

# Main execution with enhanced GPU setup and progress tracking
def main():
    # GPU device setup
    print(f"Using device: {device}")

    print("\n=======================")
    print("ðŸš€ GPU-Optimized Split Federated Learning")
    print("=======================")

    # Create directories
    # os.makedirs(os.path.dirname(config.LOG_PATH), exist_ok=True)
    # os.makedirs(config.CHECKPOINT_DIR, exist_ok=True)

    # Clear all previous checkpoints to prevent multiple checkpoints
    print("ðŸ§¹ Clearing previous checkpoints...")
    # checkpoint_manager.clear_all_checkpoints()

    # Initialize log file
    with open(config.LOG_PATH, "w") as f:
        f.write("GPU-Optimized Split Federated Learning Log\n")
        f.write("==========================================\n")
        f.write(f"Started: {datetime.now().isoformat()}\n")
        f.write(f"Device: {device}\n")
        f.write(f"Batch Size: {config.BATCH_SIZE}\n")
        f.write(f"Mixed Precision: {config.USE_MIXED_PRECISION}\n\n")

    for model_name, ModelClass in config.MODEL_LIST:
        print(f"\nðŸš€ Model: {model_name}")

        # Get the next incomplete configuration to process
        next_cut_layer, next_config_dict, next_attacker_percentage = progress_tracker.find_next_incomplete_config(model_name, ModelClass, config)

        if next_cut_layer is None:
            print(f"âœ… All configurations for {model_name} are already completed!")
            continue

        print(f"ðŸ“‚ Starting/Resuming from: Cut Layer {next_cut_layer} | {next_config_dict['label']} | {next_attacker_percentage}% attackers")

        # Get all configurations in order
        config_order = progress_tracker.get_configuration_order(model_name, ModelClass, config)
        started_processing = False

        for cut_layer, config_dict, attacker_percentage in config_order:
            num_attackers = max(0, int(config.NUM_CLIENTS * (attacker_percentage / 100)))

            # Check if we should start processing from this point
            if not started_processing:
                if (cut_layer == next_cut_layer and
                    config_dict == next_config_dict and
                    attacker_percentage == next_attacker_percentage):
                    started_processing = True
                else:
                    continue

            # Check if this configuration is already completed
            if progress_tracker.is_completed(model_name, cut_layer, config_dict["label"], attacker_percentage):
                print(f"âœ… Skipping completed configuration: {model_name} | Cut Layer {cut_layer} | {config_dict['label']} | {attacker_percentage}% attackers")
                continue

            # Set current configuration being processed
            progress_tracker.set_current_config(model_name, cut_layer, config_dict["label"], attacker_percentage)

            print(f"âš™ï¸ Processing: {model_name} | Cut Layer {cut_layer} | {config_dict['label']} | {attacker_percentage}% attackers")

            try:
                # Check if there's a checkpoint for this configuration
                checkpoint_info = checkpoint_manager.get_latest_checkpoint_info(
                    model_name, cut_layer, num_attackers, config_dict['label']
                )
                if checkpoint_info:
                    print(f"ðŸ“‚ Found checkpoint from round {checkpoint_info['round']}")

                asr, bd_acc, clean_acc = run_backdoor_experiment_optimized(
                    ModelClass, num_attackers, config_dict, cut_layer, model_name, device
                )

                # Create result dictionary
                result_dict = {
                    'model': model_name,
                    'cut_layer': cut_layer,
                    'config': config_dict['label'],
                    'attackers_percentage': attacker_percentage,
                    'clean_accuracy': clean_acc,
                    'backdoor_accuracy': bd_acc,
                    'asr': asr,
                    'timestamp': datetime.now().isoformat(),
                    'rounds_completed': config.NUM_ROUNDS,
                    'num_clients': config.NUM_CLIENTS,
                    'local_epochs': config.LOCAL_EPOCHS,
                    'learning_rate': config.LEARNING_RATE,
                    'batch_size': config.BATCH_SIZE
                }

                # Append result to CSV immediately
                csv_manager.append_result(result_dict)

                # Mark as completed after successful run
                progress_tracker.mark_completed(model_name, cut_layer, config_dict['label'], attacker_percentage)

                # Clear checkpoint after successful completion
                checkpoint_path = checkpoint_manager.get_checkpoint_path(
                    model_name, cut_layer, num_attackers, config_dict['label']
                )
                if os.path.exists(checkpoint_path):
                    os.remove(checkpoint_path)
                    print(f"ðŸ—‘ï¸ Cleared checkpoint after completion")

            except Exception as e:
                print(f"Error in experiment: {e}")
                # Continue with next configuration
                continue

    print("\nâœ… Training completed!")
    print(f"ðŸ“ˆ Progress saved to: {config.PROGRESS_FILE}")
    print(f"ðŸ’¾ Checkpoints cleared from: {config.CHECKPOINT_DIR}")
    print(f"ðŸ“Š Results appended to: {config.FINAL_OUTPUT_PATH}")

# Run the main function
if __name__ == "__main__":
    main()

