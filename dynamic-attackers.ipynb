{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "from torch.utils.data import random_split, DataLoader, Subset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Base CNN used\n",
    "# ------------------------------\n",
    "class SplitCNN(nn.Module):\n",
    "    def __init__(self, num_classes, in_channels=3):\n",
    "        super(SplitCNN, self).__init__()\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.block3 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 8 * 8, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward_until(self, x, cut_layer):\n",
    "        if cut_layer == 0:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 1:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block1(x)\n",
    "            return self.block2(x)\n",
    "        elif cut_layer == 3:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return self.block3(x)\n",
    "        elif cut_layer == -1:\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Invalid cut layer\")\n",
    "\n",
    "    def forward_from(self, x, cut_layer):\n",
    "        if cut_layer == -1:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 0:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 1:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 3:\n",
    "            pass\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# ResNet18\n",
    "# ------------------------------\n",
    "class SplitResNet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SplitResNet18, self).__init__()\n",
    "        base_model = models.resnet18(weights=None)\n",
    "        base_model.fc = nn.Linear(base_model.fc.in_features, num_classes)\n",
    "        self.block1 = nn.Sequential(base_model.conv1, base_model.bn1, base_model.relu, base_model.maxpool)\n",
    "        self.block2 = nn.Sequential(base_model.layer1, base_model.layer2)\n",
    "        self.block3 = nn.Sequential(base_model.layer3, base_model.layer4)\n",
    "        self.classifier = nn.Sequential(base_model.avgpool, nn.Flatten(), base_model.fc)\n",
    "\n",
    "    def forward_until(self, x, cut_layer):\n",
    "        if cut_layer == 0:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 1:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block1(x)\n",
    "            return self.block2(x)\n",
    "        elif cut_layer == 3:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return self.block3(x)\n",
    "        elif cut_layer == -1:\n",
    "            return x\n",
    "\n",
    "    def forward_from(self, x, cut_layer):\n",
    "        if cut_layer == -1:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 0:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 1:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block3(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# AlexNet\n",
    "# ------------------------------\n",
    "class SplitAlexNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SplitAlexNet, self).__init__()\n",
    "        base_model = models.alexnet(weights=None)\n",
    "\n",
    "        base_model.features[12] = nn.Identity()\n",
    "\n",
    "        self.block1 = nn.Sequential(*base_model.features[:3])   # Conv1 + ReLU + MaxPool\n",
    "        self.block2 = nn.Sequential(*base_model.features[3:6])  # Conv2 + ReLU + MaxPool\n",
    "        self.block3 = nn.Sequential(*base_model.features[6:])   # Remaining convs (no pool)\n",
    "\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((6, 6))       # 🌟 Critical to fix FC input size\n",
    "        base_model.classifier[6] = nn.Linear(4096, num_classes) # Adjust FC layer\n",
    "        self.classifier = base_model.classifier\n",
    "\n",
    "    def forward_until(self, x, cut_layer):\n",
    "        if cut_layer == 0 or cut_layer == 1:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 2:\n",
    "            return self.block2(self.block1(x))\n",
    "        elif cut_layer == 3:\n",
    "            return self.block3(self.block2(self.block1(x)))\n",
    "        elif cut_layer == -1:\n",
    "            return x\n",
    "\n",
    "    def forward_from(self, x, cut_layer):\n",
    "        if cut_layer == -1:\n",
    "            x = self.block3(self.block2(self.block1(x)))\n",
    "        elif cut_layer in [0, 1]:\n",
    "            x = self.block3(self.block2(x))\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 3:\n",
    "            pass  # already processed\n",
    "\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block3(self.block2(self.block1(x)))\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# DenseNet121\n",
    "# ------------------------------\n",
    "class SplitDenseNet121(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SplitDenseNet121, self).__init__()\n",
    "        base_model = models.densenet121(weights=None)\n",
    "        base_model.classifier = nn.Linear(base_model.classifier.in_features, num_classes)\n",
    "        self.block1 = base_model.features[:4]\n",
    "        self.block2 = base_model.features[4:6]\n",
    "        self.block3 = base_model.features[6:]\n",
    "        self.classifier = nn.Sequential(nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten(), base_model.classifier)\n",
    "\n",
    "    def forward_until(self, x, cut_layer):\n",
    "        if cut_layer == 0:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 1:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block1(x)\n",
    "            return self.block2(x)\n",
    "        elif cut_layer == 3:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return self.block3(x)\n",
    "        elif cut_layer == -1:\n",
    "            return x\n",
    "\n",
    "    def forward_from(self, x, cut_layer):\n",
    "        if cut_layer == -1:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 0:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 1:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block3(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return self.classifier(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# EfficientNet-B0\n",
    "# ------------------------------\n",
    "class SplitEfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SplitEfficientNetB0, self).__init__()\n",
    "        base_model = models.efficientnet_b0(weights=None)\n",
    "        base_model.classifier[1] = nn.Linear(base_model.classifier[1].in_features, num_classes)\n",
    "        self.block1 = base_model.features[:2]\n",
    "        self.block2 = base_model.features[2:5]\n",
    "        self.block3 = base_model.features[5:]\n",
    "        self.classifier = nn.Sequential(base_model.avgpool, nn.Flatten(), base_model.classifier)\n",
    "\n",
    "    def forward_until(self, x, cut_layer):\n",
    "        if cut_layer == 0:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 1:\n",
    "            return self.block1(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block1(x)\n",
    "            return self.block2(x)\n",
    "        elif cut_layer == 3:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            return self.block3(x)\n",
    "        elif cut_layer == -1:\n",
    "            return x\n",
    "\n",
    "    def forward_from(self, x, cut_layer):\n",
    "        if cut_layer == -1:\n",
    "            x = self.block1(x)\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 0:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 1:\n",
    "            x = self.block2(x)\n",
    "            x = self.block3(x)\n",
    "        elif cut_layer == 2:\n",
    "            x = self.block3(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrafficSignDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, transform=None, label_map=None):\n",
    "        self.df = df\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "        # Ensure consistent label encoding (string to int)\n",
    "        if label_map is None:\n",
    "            self.label_map = {label: idx for idx, label in enumerate(sorted(df['class'].unique()))}\n",
    "        else:\n",
    "            self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image = Image.open(row['image_path']).convert('RGB')\n",
    "        label = self.label_map[row['class']]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_dataset(csv_path, split_ratio=0.8):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df = df[~df['image_path'].str.endswith('.DS_Store')]  # remove .DS_Store rows\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    label_map = {label: idx for idx, label in enumerate(sorted(df['class'].unique()))}\n",
    "    dataset = TrafficSignDataset(df, transform=transform, label_map=label_map)\n",
    "\n",
    "    train_size = int(len(dataset) * split_ratio)\n",
    "    test_size = len(dataset) - train_size\n",
    "    trainset, testset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "    num_classes = len(label_map)\n",
    "    return trainset, testset, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset_dirichlet(dataset, num_clients, alpha):\n",
    "    # For custom datasets with label mapping\n",
    "    if hasattr(dataset.dataset, 'df'):\n",
    "        labels = [dataset.dataset.label_map[row['class']] for i in dataset.indices for _, row in dataset.dataset.df.iloc[[i]].iterrows()]\n",
    "    elif hasattr(dataset, 'targets'):\n",
    "        labels = dataset.targets\n",
    "    elif hasattr(dataset, 'labels'):\n",
    "        labels = dataset.labels\n",
    "    else:\n",
    "        raise ValueError(\"Cannot extract labels from dataset.\")\n",
    "\n",
    "    labels = np.array(labels)\n",
    "    num_classes = np.unique(labels).size\n",
    "    idx_by_class = {k: np.where(labels == k)[0] for k in range(num_classes)}\n",
    "\n",
    "    client_indices = {i: [] for i in range(num_clients)}\n",
    "    for c in range(num_classes):\n",
    "        idx_c = idx_by_class[c]\n",
    "        np.random.shuffle(idx_c)\n",
    "        proportions = np.random.dirichlet(alpha * np.ones(num_clients))\n",
    "        proportions = (np.cumsum(proportions) * len(idx_c)).astype(int)\n",
    "        proportions = np.concatenate(([0], proportions))\n",
    "        for i in range(num_clients):\n",
    "            client_indices[i].extend(idx_c[proportions[i]:proportions[i + 1]])\n",
    "\n",
    "    return client_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cut_layers_for_model(model):\n",
    "    \"\"\"\n",
    "    Dynamically determines cut layers based on model definition.\n",
    "    Also includes two extremes:\n",
    "    - 0  = full model on client\n",
    "    - -1 = client does minimal work, server does most\n",
    "    \"\"\"\n",
    "    cut_layers = []\n",
    "\n",
    "    if hasattr(model, 'forward_until'):\n",
    "        import inspect\n",
    "        src = inspect.getsource(model.forward_until)\n",
    "        for i in range(1, 5):  # change if your models go beyond cut_layer == 4\n",
    "            if f\"cut_layer == {i}\" in src:\n",
    "                cut_layers.append(i)\n",
    "\n",
    "    return [0] + cut_layers + [-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backdoor Attackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_backdoor_dynamic(data, targets, injection_rate=0.5, pattern_type=\"plus\",\n",
    "                            pattern_size=0.1, location=\"fixed\", target_label=1,\n",
    "                            color=(0.5, 0.0, 0.5)):  # Default: purple\n",
    "    \"\"\"\n",
    "    Injects a dynamic backdoor trigger into a fraction of images in the batch.\n",
    "\n",
    "    Parameters:\n",
    "      data (torch.Tensor): Batch of images, shape (B, C, H, W).\n",
    "      targets (torch.Tensor): Corresponding labels.\n",
    "      injection_rate (float): Fraction of images to modify.\n",
    "      pattern_type (str): 'plus', 'minus', 'block', or 'random'.\n",
    "      pattern_size (float): Fraction of image dimension to determine patch size.\n",
    "      location (str): 'fixed' or 'random' placement.\n",
    "      target_label (int): The label to assign to backdoor images.\n",
    "      color (tuple): RGB tuple with values between 0-1 for patch colour (e.g. purple = (0.5, 0, 0.5)).\n",
    "\n",
    "    Returns:\n",
    "      (data, targets): Modified tensors.\n",
    "    \"\"\"\n",
    "    B, C, H, W = data.shape\n",
    "    num_to_inject = int(B * injection_rate)\n",
    "    if num_to_inject == 0:\n",
    "        return data, targets\n",
    "\n",
    "    indices = torch.randperm(B)[:num_to_inject]\n",
    "\n",
    "    for i in indices:\n",
    "        ps = random.choice([0.1, 0.2, 0.3, 0.4]) if pattern_size == -1 else pattern_size\n",
    "        s = max(int(H * ps), 1)\n",
    "\n",
    "        if location == \"random\":\n",
    "            top = torch.randint(0, H - s + 1, (1,)).item()\n",
    "            left = torch.randint(0, W - s + 1, (1,)).item()\n",
    "        else:  # fixed\n",
    "            top = H - s\n",
    "            left = W - s\n",
    "\n",
    "        actual_pattern = pattern_type\n",
    "        if pattern_type == \"random\":\n",
    "            actual_pattern = random.choice([\"plus\", \"minus\", \"block\"])\n",
    "\n",
    "        r, g, b = color\n",
    "\n",
    "        if actual_pattern == \"plus\":\n",
    "            center_row = top + s // 2\n",
    "            center_col = left + s // 2\n",
    "            data[i, 0, center_row, left:left + s] = r  # Red channel horizontal\n",
    "            data[i, 1, center_row, left:left + s] = g\n",
    "            data[i, 2, center_row, left:left + s] = b\n",
    "            data[i, 0, top:top + s, center_col] = r  # Red channel vertical\n",
    "            data[i, 1, top:top + s, center_col] = g\n",
    "            data[i, 2, top:top + s, center_col] = b\n",
    "\n",
    "        elif actual_pattern == \"minus\":\n",
    "            center_row = top + s // 2\n",
    "            data[i, 0, center_row, left:left + s] = r\n",
    "            data[i, 1, center_row, left:left + s] = g\n",
    "            data[i, 2, center_row, left:left + s] = b\n",
    "\n",
    "        elif actual_pattern == \"block\":\n",
    "            data[i, 0, top:top + s, left:left + s] = r\n",
    "            data[i, 1, top:top + s, left:left + s] = g\n",
    "            data[i, 2, top:top + s, left:left + s] = b\n",
    "\n",
    "        else:  # default to plus\n",
    "            center_row = top + s // 2\n",
    "            center_col = left + s // 2\n",
    "            data[i, 0, center_row, left:left + s] = r\n",
    "            data[i, 1, center_row, left:left + s] = g\n",
    "            data[i, 2, center_row, left:left + s] = b\n",
    "            data[i, 0, top:top + s, center_col] = r\n",
    "            data[i, 1, top:top + s, center_col] = g\n",
    "            data[i, 2, top:top + s, center_col] = b\n",
    "\n",
    "        targets[i] = target_label\n",
    "\n",
    "    return data, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitfl_train_epoch(client_model, server_model, dataloader, cut_layer, lr,\n",
    "                        malicious=False, injection_rate=0.5, pattern_size=0.1,\n",
    "                        location=\"fixed\", pattern_type=\"plus\", target_label=1):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    client_model.train()\n",
    "    server_model.train()\n",
    "    optimizer = optim.SGD(list(client_model.parameters()) + list(server_model.parameters()), lr=lr)\n",
    "\n",
    "    for data, target in dataloader:\n",
    "        if malicious:\n",
    "            data, target = inject_backdoor_dynamic(data, target,\n",
    "                                                   injection_rate=injection_rate,\n",
    "                                                   pattern_type=pattern_type,\n",
    "                                                   pattern_size=pattern_size,\n",
    "                                                   location=location,\n",
    "                                                   target_label=target_label)\n",
    "            # 🔍 Save sample poisoned images once per run for debug\n",
    "            if not hasattr(splitfl_train_epoch, \"_image_logged\"):\n",
    "                from backdoor import save_backdoor_images\n",
    "                poisoned_batch, _ = inject_backdoor_dynamic(data.clone(), target.clone(),\n",
    "                                                            injection_rate=1.0,\n",
    "                                                            pattern_type=pattern_type,\n",
    "                                                            pattern_size=pattern_size,\n",
    "                                                            location=location,\n",
    "                                                            target_label=target_label)\n",
    "                save_backdoor_images(poisoned_batch, filename=f\"backdoor_sample_cut{cut_layer}.jpg\")\n",
    "                splitfl_train_epoch._image_logged = True\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        activation = client_model.forward_until(data, cut_layer)\n",
    "        output = server_model.forward_from(activation, cut_layer)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def server_aggregate_split(global_model, client_state_dicts):\n",
    "    \"\"\"\n",
    "    Aggregates the parameters of the client-side model across all clients\n",
    "    using simple averaging.\n",
    "    \"\"\"\n",
    "    global_dict = global_model.state_dict()\n",
    "    for key in global_dict.keys():\n",
    "        global_dict[key] = torch.stack([sd[key].float() for sd in client_state_dicts], 0).mean(0)\n",
    "    global_model.load_state_dict(global_dict)\n",
    "    return global_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants across Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 10\n",
    "ALPHA = 0.5\n",
    "NUM_ROUNDS = 30\n",
    "LOCAL_EPOCHS = 5\n",
    "MODEL_LIST = [\n",
    "    (\"AlexNet\", SplitAlexNet),\n",
    "    (\"DenseNet121\", SplitDenseNet121),\n",
    "    (\"EfficientNetB0\", SplitEfficientNetB0),\n",
    "    (\"SplitCNN\", SplitCNN),\n",
    "    (\"ResNet18\", SplitResNet18)\n",
    "]\n",
    "LOG_PATH = \"accuracy_log.txt\"\n",
    "INJECTION_RATE = 0.5\n",
    "PATTERN_SIZE = 0.1\n",
    "TARGET_LABEL = 1\n",
    "ATTACKER_PERCENTAGES = [0, 20, 50]\n",
    "CUT_LAYERS = [1, 2, 3]\n",
    "\n",
    "configurations = [\n",
    "    {\"label\": \"Static case\", \"location\": \"fixed\", \"pattern_type\": \"plus\"},\n",
    "    {\"label\": \"Location Invariant\", \"location\": \"random\", \"pattern_type\": \"plus\"},\n",
    "    {\"label\": \"Size Invariant\", \"location\": \"fixed\", \"pattern_type\": \"plus\", \"pattern_size\": \"random\"},\n",
    "    {\"label\": \"Pattern Invariant\", \"location\": \"fixed\", \"pattern_type\": \"random\"},\n",
    "    {\"label\": \"Random accross all\", \"location\": \"random\", \"pattern_type\": \"random\", \"pattern_size\": \"random\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training And Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_clean_experiment(ModelClass, cut_layer, num_classes):\n",
    "    print(f\"\\n▶️ Running clean experiment | Cut Layer = {cut_layer}\")\n",
    "    client_model = ModelClass(num_classes=num_classes)\n",
    "    server_model = ModelClass(num_classes=num_classes)\n",
    "\n",
    "    for rnd in range(NUM_ROUNDS):\n",
    "        print(f\"  🔁 Round {rnd + 1}/{NUM_ROUNDS}\")\n",
    "        client_state_dicts = []\n",
    "\n",
    "        for client_id in range(NUM_CLIENTS):\n",
    "            indices = client_indices[client_id]\n",
    "            if len(indices) == 0:\n",
    "                print(f\"    ⚠️ Skipping Client {client_id} (no samples)\")\n",
    "                continue\n",
    "            client_data = Subset(trainset, indices)\n",
    "            loader = DataLoader(client_data, batch_size=32, shuffle=True)\n",
    "\n",
    "            local_client = ModelClass(num_classes=num_classes)\n",
    "            local_server = ModelClass(num_classes=num_classes)\n",
    "            local_client.load_state_dict(client_model.state_dict())\n",
    "            local_server.load_state_dict(server_model.state_dict())\n",
    "\n",
    "            print(f\"    👤 Training Client {client_id}\")\n",
    "            splitfl_train_epoch(local_client, local_server, loader, cut_layer, lr=0.01,\n",
    "                                malicious=False, injection_rate=0.0)\n",
    "\n",
    "            client_state_dicts.append(local_client.state_dict())\n",
    "\n",
    "        print(\"    🧠 Aggregating client models at server\")\n",
    "        client_model = server_aggregate_split(client_model, client_state_dicts)\n",
    "\n",
    "    # Evaluation\n",
    "    print(\"  🧪 Evaluating global model...\")\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "    total = 0\n",
    "    correct_clean = 0\n",
    "\n",
    "    client_model.eval()\n",
    "    server_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            out_clean = server_model.forward_from(client_model.forward_until(data, cut_layer), cut_layer)\n",
    "            _, pred_clean = torch.max(out_clean, 1)\n",
    "            correct_clean += (pred_clean == target).sum().item()\n",
    "            total += data.size(0)\n",
    "\n",
    "    accuracy = 100 * correct_clean / total\n",
    "    print(f\"  ✅ Accuracy @ Cut Layer {cut_layer}: {accuracy:.2f}%\")\n",
    "\n",
    "    # Realtime log\n",
    "    log_accuracy_to_file(model_name, cut_layer, accuracy)\n",
    "\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_backdoor_experiment(ModelClass, num_attackers, config, cut_layer, model_name):\n",
    "    client_model = ModelClass(num_classes=num_classes)\n",
    "    server_model = ModelClass(num_classes=num_classes)\n",
    "\n",
    "    if config.get(\"pattern_size\") == \"random\":\n",
    "        pattern_size = -1\n",
    "    else:\n",
    "        pattern_size = config.get(\"pattern_size\", PATTERN_SIZE)\n",
    "\n",
    "    location = config[\"location\"]\n",
    "    pattern_type = config[\"pattern_type\"]\n",
    "\n",
    "    for rnd in range(NUM_ROUNDS):\n",
    "        client_state_dicts = []\n",
    "        malicious_clients = random.sample(range(NUM_CLIENTS), num_attackers)\n",
    "\n",
    "        for client_id in range(NUM_CLIENTS):\n",
    "            indices = client_indices[client_id]\n",
    "            client_data = Subset(trainset, indices)\n",
    "            loader = DataLoader(client_data, batch_size=32, shuffle=True)\n",
    "\n",
    "            local_client = ModelClass(num_classes=num_classes)\n",
    "            local_server = ModelClass(num_classes=num_classes)\n",
    "            local_client.load_state_dict(client_model.state_dict())\n",
    "            local_server.load_state_dict(server_model.state_dict())\n",
    "\n",
    "            is_malicious = client_id in malicious_clients\n",
    "            splitfl_train_epoch(\n",
    "                local_client, local_server, loader, cut_layer, lr=0.01,\n",
    "                malicious=is_malicious,\n",
    "                injection_rate=INJECTION_RATE,\n",
    "                pattern_size=pattern_size,\n",
    "                location=location,\n",
    "                pattern_type=pattern_type,\n",
    "                target_label=TARGET_LABEL\n",
    "            )\n",
    "\n",
    "            client_state_dicts.append(local_client.state_dict())\n",
    "\n",
    "        client_model = server_aggregate_split(client_model, client_state_dicts)\n",
    "\n",
    "    # Evaluation\n",
    "    testloader = DataLoader(testset, batch_size=32, shuffle=False)\n",
    "    total = 0\n",
    "    correct_clean = 0\n",
    "    correct_bd = 0\n",
    "    target_preds = 0\n",
    "\n",
    "    client_model.eval()\n",
    "    server_model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in testloader:\n",
    "            # Apply backdoor at eval too\n",
    "            bd_data, _ = inject_backdoor_dynamic(\n",
    "                data.clone(), target.clone(),\n",
    "                injection_rate=1.0,\n",
    "                pattern_type=pattern_type,\n",
    "                pattern_size=pattern_size,\n",
    "                location=location,\n",
    "                target_label=TARGET_LABEL\n",
    "            )\n",
    "            out_bd = server_model.forward_from(client_model.forward_until(bd_data, cut_layer), cut_layer)\n",
    "            _, pred_bd = torch.max(out_bd, 1)\n",
    "            target_preds += (pred_bd == TARGET_LABEL).sum().item()\n",
    "            correct_bd += (pred_bd == target).sum().item()\n",
    "\n",
    "            out_clean = server_model.forward_from(client_model.forward_until(data, cut_layer), cut_layer)\n",
    "            _, pred_clean = torch.max(out_clean, 1)\n",
    "            correct_clean += (pred_clean == target).sum().item()\n",
    "            total += data.size(0)\n",
    "\n",
    "    asr = 100 * target_preds / total\n",
    "    cleanAcc = 100 * correct_clean / total\n",
    "    backAcc = 100 * correct_bd / total\n",
    "\n",
    "    log_metrics_to_file(model_name, cut_layer, cleanAcc, backAcc, asr)\n",
    "    \n",
    "    return asr, backAcc, cleanAcc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_accuracy_to_file(model_name, cut_layer, accuracy):\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        f.write(f\"{model_name}, Cut Layer {cut_layer}, Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "def log_metrics_to_file(model_name, cut_layer, clean_accuracy, backdoor_accuracy, attack_success_rate):\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        f.write(f\"{model_name}, Cut Layer {cut_layer}, Clean Acc: {clean_accuracy:.2f}%, Backdoor Acc: {backdoor_accuracy:.2f}%, ASR: {attack_success_rate:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"📁 Loading dataset...\")\n",
    "csv_path = \"/Users/bristi/Desktop/Projects/Split Federated Learning/final_dataset_from_folders.csv\"\n",
    "trainset, testset, num_classes = get_custom_dataset(csv_path)\n",
    "print(f\"✅ Dataset loaded with {len(trainset)} training samples, {len(testset)} test samples, {num_classes} classes\")\n",
    "\n",
    "print(\"📦 Splitting data into clients (non-IID Dirichlet)...\")\n",
    "client_indices = split_dataset_dirichlet(trainset, NUM_CLIENTS, ALPHA)\n",
    "print(\"✅ Client data split complete.\")\n",
    "\n",
    "results_all = []\n",
    "\n",
    "print(\"\\n=======================\")\n",
    "print(\"🧼 Clean Accuracy Evaluation\")\n",
    "print(\"=======================\")\n",
    "\n",
    "with open(LOG_PATH, \"w\") as f:\n",
    "    f.write(\"Clean Accuracy Evaluation Log\\n\")\n",
    "    f.write(\"================================\\n\")\n",
    "    \n",
    "for model_name, ModelClass in MODEL_LIST:\n",
    "    print(f\"\\n🚀 Model: {model_name}\")\n",
    "    cut_layers = get_cut_layers_for_model(ModelClass(num_classes=num_classes))\n",
    "    for cut_layer in cut_layers:\n",
    "        for config in configurations:\n",
    "            for perc in ATTACKER_PERCENTAGES:\n",
    "                num_attackers = max(0, int(NUM_CLIENTS * (perc / 100)))\n",
    "                print(f\"⚙️ {model_name} | Cut Layer {cut_layer} | {config['label']} | {perc}% attackers\")\n",
    "                asr, bd_acc, clean_acc = run_backdoor_experiment(\n",
    "                    ModelClass, num_attackers, config, cut_layer, model_name\n",
    "                )\n",
    "\n",
    "# --------------------------\n",
    "# Save Final Combined Results\n",
    "# --------------------------\n",
    "final_df = pd.DataFrame(results_all)\n",
    "final_output_path = \"clean_accuracy_no_attack.csv\"\n",
    "final_df.to_csv(final_output_path, index=False)\n",
    "print(f\"\\n📊 All model results saved to {final_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
